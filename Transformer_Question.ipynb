{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divya2212001/colabs/blob/main/Transformer_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsogKx1rPnhA",
        "outputId": "a56c6b82-5881-459a-fa5a-76e0bee256a2"
      },
      "id": "UsogKx1rPnhA",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 26 04:39:22 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmyBJ5SVPqZL",
        "outputId": "d642de7e-a7d5-4bb4-fa5b-18211eeaf95d"
      },
      "id": "FmyBJ5SVPqZL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15355f23",
      "metadata": {
        "id": "15355f23"
      },
      "source": [
        "# Transformer Translation: English to Hindi\n",
        "\n",
        "**Goal:** Build a Transformer model to translate English to Hindi using **only PyTorch nn layers**.\n",
        "\n",
        "**Dataset:** `Dataset_English_Hindi.csv` containing thousands of English-Hindi translation pairs.\n",
        "\n",
        "### What You'll Learn\n",
        "- How to build a Transformer from scratch using `nn.Transformer`\n",
        "- Learned positional embeddings (like GPT-2)\n",
        "- End-to-end sequence-to-sequence translation\n",
        "- Training without padding for simplicity\n",
        "\n",
        "### Architecture Overview\n",
        "```\n",
        "English → Token Embedding → + Position Embedding → Encoder\n",
        "                                                      ↓\n",
        "Hindi ← Output Projection ← Decoder ← + Position Embedding ← Token Embedding\n",
        "```\n",
        "\n",
        "### nn Layers Used\n",
        "| Layer | Purpose |\n",
        "|-------|--------|\n",
        "| `nn.Embedding` | Token embeddings (words → vectors) |\n",
        "| `nn.Embedding` | Learned positional embeddings |\n",
        "| `nn.Transformer` | Complete encoder-decoder with attention |\n",
        "| `nn.Linear` | Output projection to vocabulary |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2ee4ef03",
      "metadata": {
        "id": "2ee4ef03",
        "outputId": "29e549d0-cd84-43c9-ed4b-d1735fde577f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.10.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn # Hint: PyTorch's neural network module\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbf2ffa",
      "metadata": {
        "id": "9bbf2ffa"
      },
      "source": [
        "## 1. Import Libraries\n",
        "\n",
        "* **Import Libraries:** Load the necessary libraries for building our Transformer model.\n",
        "\n",
        "**What each library does:**\n",
        "| Library | Description |\n",
        "|---------|-------------|\n",
        "| `torch` | Core PyTorch library for tensor operations and neural networks |\n",
        "| `torch.nn` | Neural network building blocks (layers, loss functions, etc.) |\n",
        "| `pandas` | Data manipulation library for loading CSV files |\n",
        "\n",
        "**Documentation:**\n",
        "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - Official PyTorch docs\n",
        "- [torch.nn](https://pytorch.org/docs/stable/nn.html) - Neural network module\n",
        "- [pandas](https://pandas.pydata.org/docs/) - Data analysis library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "57e5e03d",
      "metadata": {
        "id": "57e5e03d",
        "outputId": "b0142843-9501-4dc6-eb34-aca8b011c621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5000 translation pairs\n",
            "\n",
            "Sample Examples:\n",
            "1. Help!                                    → बचाओ!\n",
            "2. Jump.                                    → उछलो.\n",
            "3. Jump.                                    → कूदो.\n",
            "4. Jump.                                    → छलांग.\n",
            "5. Hello!                                   → नमस्ते।\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from CSV\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1KRn1LucS8rhz-VTZrdyb-sSDETmWCNhD\")  # Hint: Function to read CSV files\n",
        "\n",
        "# Remove  NaN values\n",
        "df = df.dropna() # Hint:  then remove duplicate rows\n",
        "\n",
        "#remove duplicate values\n",
        "df.drop_duplicates(inplace=True)  # Hint: Remove NaN rows\n",
        "\n",
        "# Convert to list of tuples\n",
        "data = list(zip(df['English'].tolist(), df['Hindi'].tolist()))\n",
        "\n",
        "# Use a subset for faster training (adjust as needed)\n",
        "MAX_SAMPLES = 5000\n",
        "data = data[:MAX_SAMPLES]\n",
        "\n",
        "print(f\"Loaded {len(data)} translation pairs\")\n",
        "print(\"\\nSample Examples:\")\n",
        "for i, (eng, hin) in enumerate(data[:5], 1):\n",
        "    print(f\"{i}. {eng[:15]:40} → {hin[:40]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c4bff5",
      "metadata": {
        "id": "14c4bff5"
      },
      "source": [
        "## 2. Load Dataset from CSV\n",
        "\n",
        "* **Load Translation Data:** This cell loads the English-Hindi dataset and displays sample translations.\n",
        "\n",
        "**Hints:**\n",
        "- Use `pd.read_csv()` to load CSV files\n",
        "- Use `.dropna()` to remove missing values\n",
        "- Use `.drop_duplicates()` to remove duplicate rows\n",
        "- Adjust `MAX_SAMPLES` to control dataset size\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `pd.read_csv('file.csv')` | Reads a CSV file and returns a DataFrame with rows and columns |\n",
        "| `df.dropna()` | Removes rows that have NaN (missing) values |\n",
        "| `df.drop_duplicates()` | Removes duplicate rows from the DataFrame |\n",
        "| `list(zip(a, b))` | Combines two lists into list of tuples [(a1,b1), (a2,b2), ...] |\n",
        "| `len(data)` | Returns the number of items in the list |\n",
        "\n",
        "**Documentation:**\n",
        "- [pd.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) - Read CSV files\n",
        "- [DataFrame.dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) - Remove missing values\n",
        "- [DataFrame.drop_duplicates](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) - Remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6fb67e3a",
      "metadata": {
        "id": "6fb67e3a",
        "outputId": "c821ed8b-45a8-492b-c7d6-0f561e836b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocab: 11213 words\n",
            "Hindi vocab: 10865 words\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(sentences):\n",
        "    vocab = {\"<SOS>\": 0, \"<EOS>\": 1, \"<UNK>\": 2}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():  # Hint: Split sentence into words\n",
        "            if word not in vocab:  # Hint: Check if word is NOT in dictionary\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "# Create vocabularies\n",
        "eng_vocab = build_vocab([p[0] for p in data])\n",
        "hin_vocab = build_vocab([p[1] for p in data])\n",
        "\n",
        "# Reverse mappings\n",
        "eng_idx2word = {v: k for k, v in eng_vocab.items()}  # Hint: Get key-value pairs\n",
        "hin_idx2word = {v: k for k, v in hin_vocab.items()}  # Hint: Get key-value pairs\n",
        "\n",
        "print(f\"English vocab: {len(eng_vocab)} words\")\n",
        "print(f\"Hindi vocab: {len(hin_vocab)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5474d4ed",
      "metadata": {
        "id": "5474d4ed"
      },
      "source": [
        "## 3. Build Vocabulary\n",
        "\n",
        "* **Create Word-to-Index Mappings:** This cell builds vocabularies that map words to unique numbers.\n",
        "\n",
        "**Hints:**\n",
        "- Start with special tokens: `<SOS>=0` (start), `<EOS>=1` (end), `<UNK>=2` (unknown)\n",
        "- Use `dict.get(word, default)` for safe dictionary lookup\n",
        "- Build reverse mapping with dictionary comprehension\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `dict.get(key, default)` | Gets value for key if exists, otherwise returns default value |\n",
        "| `str.split()` | Splits string into list of words by whitespace |\n",
        "| `len(vocab)` | Returns number of unique words in vocabulary |\n",
        "| `{v: k for k, v in dict.items()}` | Creates reverse mapping (swap keys and values) |\n",
        "\n",
        "**Special Tokens:**\n",
        "| Token | Index | Purpose |\n",
        "|-------|-------|----------|\n",
        "| `<SOS>` | 0 | Start of sentence - signals beginning of translation |\n",
        "| `<EOS>` | 1 | End of sentence - signals translation is complete |\n",
        "| `<UNK>` | 2 | Unknown word - used for words not in vocabulary |\n",
        "\n",
        "**Documentation:**\n",
        "- [dict.get](https://docs.python.org/3/library/stdtypes.html#dict.get) - Safe dictionary lookup\n",
        "- [str.split](https://docs.python.org/3/library/stdtypes.html#str.split) - Split string\n",
        "- [Dictionary comprehension](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) - Create dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1cafa4",
      "metadata": {
        "id": "5a1cafa4"
      },
      "source": [
        "## 4. Convert Sentences to Tensors\n",
        "\n",
        "* **Text to Numbers:** This cell converts sentences from words into sequences of numbers (indices).\n",
        "\n",
        "**Hints:**\n",
        "- Use `vocab.get(word, 2)` to lookup word index (2 is UNK token)\n",
        "- Truncate long sentences to `MAX_SEQ_LEN - 2` (leave room for special tokens)\n",
        "- Add `<SOS>` at start of Hindi, `<EOS>` at end of both\n",
        "- Use `torch.tensor()` to create PyTorch tensors\n",
        "- Use `list.append()` to add items to list\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `str.split()` | Splits sentence into list of words |\n",
        "| `vocab.get(word, 2)` | Looks up word's index, returns 2 (UNK) if word not found |\n",
        "| `list[:n]` | Truncates list to first n elements |\n",
        "| `list.append(item)` | Adds item to end of list |\n",
        "| `torch.tensor(data, dtype)` | Creates PyTorch tensor from Python list |\n",
        "| `tensor.tolist()` | Converts tensor back to Python list for printing |\n",
        "\n",
        "**Why truncate?**\n",
        "Positional embeddings have a maximum length (`MAX_SEQ_LEN=200`). Longer sequences would cause an error.\n",
        "\n",
        "**Documentation:**\n",
        "- [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html) - Create tensors\n",
        "- [Tensor.tolist](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html) - Tensor to list\n",
        "- [list.append](https://docs.python.org/3/tutorial/datastructures.html) - Add to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9b12a549",
      "metadata": {
        "id": "9b12a549",
        "outputId": "1585c027-4034-4441-9150-56d5bdf513e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 5000 sentence pairs to tensors\n",
            "Max sequence length: 200\n",
            "Example: 'Help!' → [3, 1]\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQ_LEN = 200\n",
        "\n",
        "def sentence_to_tensor(sentence, vocab, add_sos=False, add_eos=True):\n",
        "    tokens = str(sentence).strip().split()\n",
        "    indices = [vocab.get(word, 2) for word in tokens]  # Hint: Safe dict lookup with default\n",
        "\n",
        "    # Truncate if too long (leave room for SOS/EOS tokens)\n",
        "    max_tokens = MAX_SEQ_LEN - 2\n",
        "    if len(indices) > max_tokens:\n",
        "        indices = indices[:max_tokens]\n",
        "\n",
        "    if add_sos:\n",
        "        indices = [0] + indices  # SOS token index is 0\n",
        "    if add_eos:\n",
        "        indices.append(1)  # Hint: Add item to end of list (EOS token)\n",
        "    return torch.tensor(indices, dtype=torch.long)  # Hint: Create PyTorch tensor\n",
        "\n",
        "# Convert all data\n",
        "pairs = []\n",
        "for eng, hin in data:\n",
        "    src = sentence_to_tensor(eng, eng_vocab, add_sos=False, add_eos=True)\n",
        "    tgt = sentence_to_tensor(hin, hin_vocab, add_sos=True, add_eos=True)\n",
        "    pairs.append(    (src, tgt))  # Hint: Add tuple to list\n",
        "\n",
        "print(f\"Converted {len(pairs)} sentence pairs to tensors\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LEN}\")\n",
        "print(f\"Example: '{data[0][0]}' → {pairs[0][0].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a20f5c",
      "metadata": {
        "id": "c9a20f5c"
      },
      "source": [
        "## 5. Build Transformer Model\n",
        "\n",
        "* **Define Architecture:** This cell creates the complete Transformer model using PyTorch's `nn` layers.\n",
        "\n",
        "**Hints:**\n",
        "- Use `nn.Embedding` for both token and position embeddings\n",
        "- Use `nn.Transformer` for the encoder-decoder architecture\n",
        "- Use `nn.Linear` for the final output projection\n",
        "- Use `torch.arange()` to create position indices\n",
        "\n",
        "**What each layer does:**\n",
        "| Layer | Description |\n",
        "|-------|-------------|\n",
        "| `nn.Module` | Base class for all neural networks in PyTorch |\n",
        "| `nn.Embedding(vocab_size, d_model)` | Converts word indices to dense vectors of size d_model |\n",
        "| `nn.Transformer(...)` | Complete encoder-decoder with multi-head attention |\n",
        "| `nn.Linear(d_model, vocab_size)` | Projects hidden states to vocabulary probabilities |\n",
        "| `torch.arange(n, device)` | Creates tensor [0, 1, 2, ..., n-1] on specified device |\n",
        "| `nn.Transformer.generate_square_subsequent_mask()` | Creates causal mask (prevents looking at future tokens) |\n",
        "\n",
        "**Transformer Components:**\n",
        "1. **Token Embeddings**: Convert word indices to vectors\n",
        "2. **Position Embeddings**: Add position information (learned, not fixed)\n",
        "3. **Encoder**: Processes English sentence with self-attention\n",
        "4. **Decoder**: Generates Hindi translation with cross-attention to encoder\n",
        "5. **Output Projection**: Converts hidden states to word probabilities\n",
        "\n",
        "**Documentation:**\n",
        "- [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) - Base neural network class\n",
        "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) - Embedding layer\n",
        "- [nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) - Transformer model\n",
        "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) - Linear layer\n",
        "- [torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html) - Create sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "84605fc3",
      "metadata": {
        "id": "84605fc3",
        "outputId": "c43f695d-134e-44c3-90e4-b0ff531bc7f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created with 8,036,337 parameters\n",
            "Using device: cuda\n",
            "\n",
            "nn Layers used:\n",
            "  - nn.Embedding (token + position embeddings)\n",
            "  - nn.Transformer (encoder-decoder)\n",
            "  - nn.Linear (output projection)\n",
            "  - nn.Dropout\n"
          ]
        }
      ],
      "source": [
        "class TranslationTransformer(nn.Module):  # Hint: Base class for PyTorch models\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=128, nhead=4,\n",
        "                 num_layers=3, dim_feedforward=256, dropout=0.1, max_len=200):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Token Embeddings (nn.Embedding)\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)  # Hint: Converts indices to vectors\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)  # Hint: Same layer type\n",
        "\n",
        "        # Learned Positional Embeddings (nn.Embedding) - like GPT-2\n",
        "        self.src_pos_embedding = nn.Embedding(max_len, d_model)  # Hint: Position embeddings\n",
        "        self.tgt_pos_embedding = nn.Embedding(max_len, d_model)  # Hint: Same layer type\n",
        "\n",
        "\n",
        "        # PyTorch's built-in Transformer (nn.Transformer)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )  # Hint: Complete encoder-decoder architecture\n",
        "\n",
        "        # Output projection (nn.Linear)\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)  # Hint: Fully connected layer\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_seq_len = src.size(1)\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "\n",
        "        # Create position indices\n",
        "        src_positions = torch.arange(src_seq_len, device=src.device).unsqueeze(0)  # Hint: Create sequence 0,1,2,...\n",
        "        tgt_positions = torch.arange(tgt_seq_len, device=tgt.device).unsqueeze(0)  # Hint: Same function\n",
        "\n",
        "        # Token embeddings + Positional embeddings\n",
        "        src_emb = self.src_embedding(src) + self.src_pos_embedding(src_positions)\n",
        "        tgt_emb = self.tgt_embedding(tgt) + self.tgt_pos_embedding(tgt_positions)\n",
        "\n",
        "        # Create causal mask for decoder (prevents looking at future tokens)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
        "\n",
        "        # Transformer forward pass (no padding masks needed)\n",
        "        output = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
        "\n",
        "        return self.fc_out(output)\n",
        "\n",
        "# Create model\n",
        "model = TranslationTransformer(\n",
        "    src_vocab_size=len(eng_vocab),\n",
        "    tgt_vocab_size=len(hin_vocab),\n",
        "    d_model=128,\n",
        "    nhead=4,\n",
        "    num_layers=3,\n",
        "    dim_feedforward=256,\n",
        "    dropout=0.1,\n",
        "    max_len=MAX_SEQ_LEN\n",
        ")\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model created with {total_params:,} parameters\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"\\nnn Layers used:\")\n",
        "print(f\"  - nn.Embedding (token + position embeddings)\")\n",
        "print(f\"  - nn.Transformer (encoder-decoder)\")\n",
        "print(f\"  - nn.Linear (output projection)\")\n",
        "print(f\"  - nn.Dropout\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105d3698",
      "metadata": {
        "id": "105d3698"
      },
      "source": [
        "## 6. Train the Model\n",
        "\n",
        "* **Training Loop:** This cell trains the Transformer model on the translation pairs.\n",
        "\n",
        "**Hints:**\n",
        "- Use `nn.CrossEntropyLoss()` to measure prediction error\n",
        "- Use `torch.optim.Adam()` for adaptive learning rate optimization\n",
        "- Use `optimizer.zero_grad()` before each forward pass\n",
        "- Use `loss.backward()` to compute gradients\n",
        "- Use `optimizer.step()` to update weights\n",
        "- Use `torch.nn.utils.clip_grad_norm_()` to prevent exploding gradients\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `nn.CrossEntropyLoss()` | Loss function that measures how wrong predictions are |\n",
        "| `torch.optim.Adam(params, lr)` | Optimizer that updates weights; lr=learning rate |\n",
        "| `optimizer.zero_grad()` | Clears old gradients (required before each training step) |\n",
        "| `model(src, tgt)` | Forward pass through the model |\n",
        "| `loss.backward()` | Computes gradients via backpropagation |\n",
        "| `torch.nn.utils.clip_grad_norm_(params, max)` | Prevents gradient explosion by clipping |\n",
        "| `optimizer.step()` | Updates model weights using computed gradients |\n",
        "| `tensor.unsqueeze(0)` | Adds batch dimension (e.g., [5] → [1,5]) |\n",
        "\n",
        "**Training Process:**\n",
        "```\n",
        "For each epoch:\n",
        "  For each sentence pair:\n",
        "    1. Forward pass → get predictions\n",
        "    2. Calculate loss (error)\n",
        "    3. Backward pass → compute gradients\n",
        "    4. Update weights\n",
        "```\n",
        "\n",
        "**Documentation:**\n",
        "- [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) - Loss function\n",
        "- [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) - Adam optimizer\n",
        "- [Tensor.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) - Backpropagation\n",
        "- [clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) - Gradient clipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c3d75d5b",
      "metadata": {
        "id": "c3d75d5b",
        "outputId": "8fb119a8-1f9f-4f29-dc12-2f875a626636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1 epochs on 5000 pairs...\n",
            "\n",
            "Epoch   1: Loss = 6.7454\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Hint: Loss function for classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Hint: Adaptive optimizer\n",
        "\n",
        "def train_step(src, tgt):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Hint: Clear old gradients\n",
        "\n",
        "    # Decoder input: all except last token\n",
        "    # Target: all except first token (SOS)\n",
        "    tgt_input = tgt[:, :-1]\n",
        "    tgt_output = tgt[:, 1:]\n",
        "\n",
        "    output = model(src, tgt_input)\n",
        "    loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "    loss.backward()  # Hint: Compute gradients via backpropagation\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Prevent exploding gradients\n",
        "    optimizer.step()  # Hint: Update weights\n",
        "    return loss.item()\n",
        "\n",
        "# Training loop (sample by sample - no padding needed)\n",
        "EPOCHS = 1  # More epochs for better convergence\n",
        "print(f\"Training for {EPOCHS} epochs on {len(pairs)} pairs...\\n\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    total_loss = 0\n",
        "    for src, tgt in pairs:\n",
        "        src = src.unsqueeze(0).to(device)  # Hint: Add batch dimension\n",
        "        tgt = tgt.unsqueeze(0).to(device)  # Hint: Same function\n",
        "        loss = train_step(src, tgt)\n",
        "        total_loss += loss\n",
        "\n",
        "    avg_loss = total_loss / len(pairs)\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:3d}: Loss = {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62c39bb",
      "metadata": {
        "id": "a62c39bb"
      },
      "source": [
        "## 7. Translation Function\n",
        "\n",
        "* **Define Inference:** This cell creates a function to translate English sentences to Hindi.\n",
        "\n",
        "**Hints:**\n",
        "- Use `model.eval()` to switch to evaluation mode\n",
        "- Use `torch.no_grad()` to disable gradient computation (faster)\n",
        "- Start with `<SOS>` token (index 0)\n",
        "- Use `torch.softmax()` to convert logits to probabilities\n",
        "- Use `.argmax()` to get the most likely word\n",
        "- Stop when `<EOS>` token (index 1) is predicted\n",
        "- Use `torch.cat()` to append new token to sequence\n",
        "\n",
        "**What each function does:**\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `model.eval()` | Switch to evaluation mode (disables dropout, etc.) |\n",
        "| `torch.no_grad()` | Context manager that disables gradient tracking (faster, less memory) |\n",
        "| `torch.softmax(x, dim)` | Converts logits to probabilities (sum to 1.0) |\n",
        "| `tensor.argmax(dim)` | Returns index of highest value (the prediction) |\n",
        "| `tensor.item()` | Converts single-element tensor to Python number |\n",
        "| `dict.get(key, default)` | Safe dictionary lookup, returns default if key not found |\n",
        "| `torch.cat([a, b], dim)` | Concatenates tensors along specified dimension |\n",
        "| `' '.join(list)` | Combines list of words into single string with spaces |\n",
        "\n",
        "**Translation Process:**\n",
        "```\n",
        "1. Encode English sentence\n",
        "2. Start with <SOS> token\n",
        "3. Loop:\n",
        "   - Predict next Hindi word\n",
        "   - If <EOS>, stop\n",
        "   - Otherwise, append word and continue\n",
        "4. Return Hindi sentence\n",
        "```\n",
        "\n",
        "**Documentation:**\n",
        "- [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html) - Disable gradients\n",
        "- [torch.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) - Softmax function\n",
        "- [Tensor.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html) - Get max index\n",
        "- [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html) - Concatenate tensors\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "258f6e1c",
      "metadata": {
        "id": "258f6e1c",
        "outputId": "9c96360f-c3d1-4c4b-e1e5-f2b686a2b57b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation Results (random samples):\n",
            "================================================================================\n",
            "EN: Awesome!                                \n",
            "HI: बहुत बढ़िया!\n",
            "PR: समालोचना\n",
            "--------------------------------------------------------------------------------\n",
            "EN: Come in.                                \n",
            "HI: अंदर आ जाओ।\n",
            "PR: इस\n",
            "--------------------------------------------------------------------------------\n",
            "EN: Get out!                                \n",
            "HI: बाहर निकल जाओ!\n",
            "PR: इस\n",
            "--------------------------------------------------------------------------------\n",
            "EN: Go away!                                \n",
            "HI: चले जाओ!\n",
            "PR: इस\n",
            "--------------------------------------------------------------------------------\n",
            "EN: Goodbye!                                \n",
            "HI: ख़ुदा हाफ़िज़।\n",
            "PR: समालोचना\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def translate(sentence, max_len=50):\n",
        "    model.eval()  # Hint: Switch to evaluation mode\n",
        "    src = sentence_to_tensor(sentence, eng_vocab).unsqueeze(0).to(device)\n",
        "    tgt = torch.tensor([[0]]).to(device)  # Start with SOS token\n",
        "\n",
        "    words = []\n",
        "    with torch.no_grad():  # Hint: Disable gradient tracking\n",
        "        for i in range(max_len):\n",
        "            output = model(src, tgt)\n",
        "\n",
        "            # Get probabilities and pick the most likely token\n",
        "            probs = torch.softmax(output[:, -1, :], dim=-1)  # Hint: Convert to probabilities\n",
        "            next_token = probs.argmax(dim=-1)  # Hint: Get index of max value\n",
        "\n",
        "            # Stop if EOS is predicted (but not on first token if we have nothing)\n",
        "            if next_token.item() == 1 and len(words) > 0:\n",
        "                break\n",
        "\n",
        "            # Skip EOS/SOS tokens in output\n",
        "            if next_token.item() not in [0, 1]:\n",
        "                words.append(hin_idx2word.get(next_token.item(), '<UNK>'))\n",
        "\n",
        "            tgt = torch.cat(    [tgt, next_token.unsqueeze(0)], dim=1)  # Hint: Concatenate tensors\n",
        "\n",
        "            # Safety: stop if we keep getting special tokens\n",
        "            if i > 5 and len(words) == 0:\n",
        "                break\n",
        "\n",
        "    return ' '.join(words) if words else \"(no translation)\"  # Hint: Join list into string\n",
        "\n",
        "# Test on random samples from the dataset\n",
        "import random\n",
        "test_samples = data[10:15]\n",
        "\n",
        "print(\"Translation Results (random samples):\")\n",
        "print(\"=\" * 80)\n",
        "for eng, hin in test_samples:\n",
        "    pred = translate(eng)\n",
        "    print(f\"EN: {eng[:40]:40}\")\n",
        "    print(f\"HI: {hin[:50]}\")\n",
        "    print(f\"PR: {pred[:50]}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7016822",
      "metadata": {
        "id": "b7016822"
      },
      "source": [
        "## 8. Try Your Own Translations!\n",
        "\n",
        "* **Interactive Testing:** Try translating your own English sentences to Hindi.\n",
        "\n",
        "**Hints:**\n",
        "- Modify `test_sentences` to add your own examples\n",
        "- Model works best on short, simple sentences\n",
        "- Words must be in the vocabulary (trained on dataset)\n",
        "\n",
        "**Best practices:**\n",
        "- Use simple grammar\n",
        "- Keep sentences short (< 10 words)\n",
        "- Use common words from everyday conversation\n",
        "\n",
        "**Documentation:**\n",
        "- Experiment with different sentence structures\n",
        "- Compare translations to see what the model learned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "995af1ef",
      "metadata": {
        "id": "995af1ef",
        "outputId": "07ef7b6e-de7c-4adf-e36a-a8f05cebc110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Translations:\n",
            "============================================================\n",
            "Input:  Hello!\n",
            "Output: समालोचना\n",
            "------------------------------------------------------------\n",
            "Input:  How are you?\n",
            "Output: इस\n",
            "------------------------------------------------------------\n",
            "Input:  Thank you.\n",
            "Output: इस\n",
            "------------------------------------------------------------\n",
            "Input:  Good morning!\n",
            "Output: यह\n",
            "------------------------------------------------------------\n",
            "Input:  What is the issue\n",
            "Output: यह\n",
            "------------------------------------------------------------\n",
            "\n",
            "Vocabulary sizes:\n",
            "  English: 11213 words\n",
            "  Hindi: 10865 words\n"
          ]
        }
      ],
      "source": [
        "# Try your own translation\n",
        "test_sentences = [\n",
        "    \"Hello!\",\n",
        "    \"How are you?\",\n",
        "    \"Thank you.\",\n",
        "    \"Good morning!\",\n",
        "    \"What is the issue\"\n",
        "]\n",
        "\n",
        "print(\"Custom Translations:\")\n",
        "print(\"=\" * 60)\n",
        "for test in test_sentences:\n",
        "    result = translate(test)  # Hint: Call the translation function\n",
        "    print(f\"Input:  {test}\")\n",
        "    print(f\"Output: {result}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\nVocabulary sizes:\")\n",
        "print(f\"  English: {len(eng_vocab)} words\")\n",
        "print(f\"  Hindi: {len(hin_vocab)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610648fd",
      "metadata": {
        "id": "610648fd"
      },
      "source": [
        "\n",
        "## Summary\n",
        "\n",
        "### What We Built\n",
        "**A complete Transformer-based English-Hindi translation system using pure PyTorch `nn` layers!**\n",
        "\n",
        "### How It Works\n",
        "```\n",
        "English: \"Hello\"\n",
        "   ↓\n",
        "Tokens: [word_idx, EOS]  \n",
        "   ↓\n",
        "Embeddings: [128-dim vectors]\n",
        "   ↓\n",
        "+ Position: [learned position vectors]\n",
        "   ↓\n",
        "Encoder: [multi-head attention + feedforward]\n",
        "   ↓\n",
        "Context Vector: [encoded meaning]\n",
        "   ↓\n",
        "Decoder: [generates Hindi word-by-word]\n",
        "   ↓\n",
        "Hindi: \"नमस्ते\"\n",
        "```\n",
        "\n",
        "### PyTorch `nn` Layers Used\n",
        "| Layer | Purpose |\n",
        "|-------|--------|\n",
        "| `nn.Embedding` | Token embeddings (words → vectors) |\n",
        "| `nn.Embedding` | Learned positional embeddings |\n",
        "| `nn.Transformer` | Complete encoder-decoder with attention |\n",
        "| `nn.Linear` | Project to vocabulary size |\n",
        "| `nn.CrossEntropyLoss` | Training loss function |\n",
        "\n",
        "### Key Concepts\n",
        "1. **Token Embeddings**: Words → Dense vectors\n",
        "2. **Positional Embeddings**: Position → Learned vectors (like GPT-2)\n",
        "3. **Attention**: Model can \"look at\" any input word when generating output\n",
        "4. **Causal Masking**: Decoder can't cheat by looking at future output\n",
        "5. **Autoregressive**: Generate one word at a time\n",
        "\n",
        "### Training Tips\n",
        "- Adjust `MAX_SAMPLES` to train on more/fewer examples\n",
        "- Increase `EPOCHS` for better convergence\n",
        "- Use GPU (`cuda`) for faster training on large datasets\n",
        "\n",
        "### Resources\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper\n",
        "- [PyTorch nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) - Official docs\n",
        "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual guide\n",
        "- [GPT-2 Positional Embeddings](https://openai.com/blog/better-language-models/) - Learned positions\n",
        "\n",
        "**Congratulations!** You built a Transformer translation system using pure PyTorch nn layers!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "655196e6",
      "metadata": {
        "id": "655196e6"
      },
      "source": [
        "# Simple Encoder-Decoder Translation\n",
        "\n",
        "**Goal:** Build a simple encoder-decoder model to translate English to Hindi.\n",
        "\n",
        "### Exercise: Build an Encoder-Decoder Neural Network\n",
        "\n",
        "In this exercise, you will build a sequence-to-sequence model using PyTorch to translate English sentences to Hindi. Fill in the blanks to complete the code.\n",
        "\n",
        "This notebook demonstrates:\n",
        "- How encoder processes input text\n",
        "- How decoder generates output text\n",
        "- Complete translation pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fad5a53",
      "metadata": {
        "id": "8fad5a53"
      },
      "source": [
        "## What is Encoder-Decoder?\n",
        "\n",
        "Think of translation as a two-step process:\n",
        "\n",
        "1. **Encoder**: Reads the English sentence and creates a \"summary\" (context vector)\n",
        "2. **Decoder**: Uses that summary to write the Hindi sentence\n",
        "\n",
        "```\n",
        "English → [ENCODER] → Context Vector → [DECODER] → Hindi\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2324617c",
      "metadata": {
        "id": "2324617c"
      },
      "source": [
        "* **Import Libraries:** This cell imports the necessary PyTorch libraries for building neural networks.\n",
        "\n",
        "**Hints:**\n",
        "- Import `torch.nn` as `nn` for neural network layers\n",
        "- Import `torch.nn.functional` as `F` for activation functions\n",
        "- torch is the main PyTorch library\n",
        "\n",
        "**Documentation:**\n",
        "- [torch.nn](https://pytorch.org/docs/stable/nn.html)\n",
        "- [torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html)\n",
        "- [PyTorch Getting Started](https://pytorch.org/get-started/locally/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch         # Hint: main PyTorch library\n",
        "import torch.nn as nn   # Hint: neural network module (contains layers like Linear, LSTM, etc.)\n",
        "import torch.nn.functional as F  # Hint: functional API (contains relu, softmax, cross_entropy, etc.)\n",
        "\n",
        "print(\"Libraries loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjH11Kz8uYK_",
        "outputId": "5ad97353-4753-4cbe-c350-6ac47492b07d"
      },
      "id": "wjH11Kz8uYK_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea2f7e8",
      "metadata": {
        "id": "5ea2f7e8"
      },
      "source": [
        "## Step 1: Prepare Data\n",
        "\n",
        "We'll use a small set of English-Hindi sentence pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd6ee52",
      "metadata": {
        "id": "9fd6ee52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf3559d-c03c-4301-d3a6-280a58a26a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Examples:\n",
            "1. I am happy      → मैं खुश हूं\n",
            "2. you are good    → तुम अच्छे हो\n",
            "3. he is smart     → वह होशियार है\n",
            "4. she is kind     → वह दयालु है\n",
            "5. we are friends  → हम दोस्त हैं\n"
          ]
        }
      ],
      "source": [
        "# Training data: English-Hindi pairs\n",
        "data = [\n",
        "    (\"I am happy\", \"मैं खुश हूं\"),\n",
        "    (\"you are good\", \"तुम अच्छे हो\"),\n",
        "    (\"he is smart\", \"वह होशियार है\"),\n",
        "    (\"she is kind\", \"वह दयालु है\"),\n",
        "    (\"we are friends\", \"हम दोस्त हैं\"),\n",
        "]\n",
        "\n",
        "print(\"Training Examples:\")\n",
        "for i, (eng, hin) in enumerate(data, 1):\n",
        "    print(f\"{i}. {eng:15} → {hin}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ed355e9",
      "metadata": {
        "id": "6ed355e9"
      },
      "source": [
        "## Step 2: Build Vocabulary\n",
        "\n",
        "* **Create Vocabulary:** This cell builds word-to-index mappings for both English and Hindi.\n",
        "\n",
        "**Hints:**\n",
        "- Special tokens: PAD (padding), SOS (start of sentence), EOS (end of sentence), UNK (unknown word)\n",
        "- Use a dictionary to map words to indices\n",
        "- Use `split()` to break sentences into words\n",
        "- Use `len(vocab)` to get the next available index\n",
        "\n",
        "**Documentation:**\n",
        "- [Python dictionaries](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)\n",
        "- [String.split](https://docs.python.org/3/library/stdtypes.html#str.split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea4f163",
      "metadata": {
        "id": "fea4f163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8415768-272f-404a-cc0e-02c527917ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary: 17 words\n",
            "Hindi vocabulary: 17 words\n",
            "\n",
            "English words: ['<PAD>', '<SOS>', '<EOS>', '<UNK>', 'I', 'am', 'happy', 'you', 'are', 'good', 'he', 'is', 'smart', 'she', 'kind', 'we', 'friends']\n",
            "\n",
            "Hindi words: ['<PAD>', '<SOS>', '<EOS>', '<UNK>', 'मैं', 'खुश', 'हूं', 'तुम', 'अच्छे', 'हो', 'वह', 'होशियार', 'है', 'दयालु', 'हम', 'दोस्त', 'हैं']\n"
          ]
        }
      ],
      "source": [
        "# Special tokens\n",
        "PAD = 0  # Padding\n",
        "SOS = 1  # Start of sentence\n",
        "EOS = 2  # End of sentence\n",
        "UNK = 3  # Unknown word\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<PAD>\": PAD, \"<SOS>\": SOS, \"<EOS>\": EOS, \"<UNK>\": UNK}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():   # Hint: method used to break sentence into words\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)   # Hint 1: key should be the current word\n",
        "                                             # Hint 2: assign next available index using current vocab size\n",
        "    return vocab\n",
        "\n",
        "# Create vocabularies\n",
        "english_sentences = [pair[0] for pair in data]\n",
        "hindi_sentences = [pair[1] for pair in data]\n",
        "\n",
        "eng_vocab = build_vocab(english_sentences)   # Hint: pass the list of English sentences\n",
        "hin_vocab = build_vocab(hindi_sentences)\n",
        "\n",
        "# Reverse mapping (number → word)\n",
        "eng_idx2word = {v: k for k, v in eng_vocab.items()}\n",
        "hin_idx2word = {v: k for k, v in hin_vocab.items()}\n",
        "\n",
        "print(f\"English vocabulary: {len(eng_vocab)} words\")\n",
        "print(f\"Hindi vocabulary: {len(hin_vocab)} words\")\n",
        "print(f\"\\nEnglish words: {list(eng_vocab.keys())}\")\n",
        "print(f\"\\nHindi words: {list(hin_vocab.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595ad965",
      "metadata": {
        "id": "595ad965"
      },
      "source": [
        "## Step 3: Convert Sentences to Numbers\n",
        "\n",
        "* **Sentence to Indices:** This cell converts text sentences into sequences of numerical indices.\n",
        "\n",
        "**Hints:**\n",
        "- Use `vocab.get(word, UNK)` to handle unknown words gracefully\n",
        "- Append EOS token at the end of each sequence\n",
        "- Use `torch.tensor()` to create PyTorch tensors\n",
        "\n",
        "**Documentation:**\n",
        "- [dict.get](https://docs.python.org/3/library/stdtypes.html#dict.get)\n",
        "- [torch.tensor](https://pytorch.org/docs/stable/tensors.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8e7e2b",
      "metadata": {
        "id": "ae8e7e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb765ec1-dbc1-43d3-d350-af99ad1b90bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example conversion:\n",
            "English: I am happy\n",
            "Indices: [4, 5, 6, 2]\n",
            "Hindi: मैं खुश हूं\n",
            "Indices: [4, 5, 6, 2]\n"
          ]
        }
      ],
      "source": [
        "def sentence_to_indices(sentence, vocab):\n",
        "    \"\"\"Convert sentence to list of word indices\"\"\"\n",
        "    indices = [vocab.get(word, UNK) for word in sentence.split()]\n",
        "    # Hint 1: dictionary method that safely retrieves value\n",
        "    # Hint 2: default value should be the index for unknown words (UNK)\n",
        "\n",
        "    indices.append(EOS)\n",
        "    # Hint 3: add special end-of-sentence token\n",
        "    # Hint 4: method used to add an element to a list\n",
        "    # Hint 5: token to mark end of sentence\n",
        "\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Convert all data\n",
        "pairs = []\n",
        "for eng, hin in data:\n",
        "    eng_tensor = sentence_to_indices(eng, eng_vocab)\n",
        "    # Hint 6: pass the English vocabulary here\n",
        "\n",
        "    hin_tensor = sentence_to_indices(hin, hin_vocab)\n",
        "    pairs.append((eng_tensor, hin_tensor))\n",
        "\n",
        "print(\"Example conversion:\")\n",
        "print(f\"English: {data[0][0]}\")\n",
        "print(f\"Indices: {pairs[0][0].tolist()}\")\n",
        "print(f\"Hindi: {data[0][1]}\")\n",
        "print(f\"Indices: {pairs[0][1].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15bcb417",
      "metadata": {
        "id": "15bcb417"
      },
      "source": [
        "## Step 4: Build Encoder\n",
        "\n",
        "* **Encoder Architecture:** This cell defines the encoder neural network that processes input sentences.\n",
        "\n",
        "**Hints:**\n",
        "- Use `nn.Embedding` to convert word indices to dense vectors\n",
        "- Use `nn.GRU` (Gated Recurrent Unit) for sequence processing\n",
        "- The GRU returns outputs and hidden state; we only need the hidden state (context vector)\n",
        "- Set `batch_first=True` for easier data handling\n",
        "\n",
        "**Documentation:**\n",
        "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "- [nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "- [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81cf09f3",
      "metadata": {
        "id": "81cf09f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725135f5-3954-48f2-b105-a5f01b6531d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder created!\n",
            "- Input: English words\n",
            "- Output: Context vector of size 32\n"
          ]
        }
      ],
      "source": [
        "class SimpleEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # Hint 1: Layer that converts word indices → dense vectors\n",
        "\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        # Hint 2: Recurrent layer type (simpler than LSTM)\n",
        "        # Hint 3: Set to True if input shape is (batch_size, seq_len, features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: input word indices\n",
        "        embedded = self.embedding(x)\n",
        "        # Hint 4: Use embedding layer defined above\n",
        "\n",
        "        _, hidden = self.gru(embedded)\n",
        "        # Hint 5: Pass embeddings through GRU\n",
        "        # Hint 6: We only need the final hidden state\n",
        "\n",
        "        return hidden\n",
        "        # Hint 7: Return the context vector (final hidden state)\n",
        "\n",
        "# Create encoder\n",
        "encoder = SimpleEncoder(\n",
        "    vocab_size=len(eng_vocab),\n",
        "    # Hint 8: Use English vocabulary here\n",
        "\n",
        "    embed_size=16,\n",
        "    hidden_size=32\n",
        ")\n",
        "\n",
        "print(\"Encoder created!\")\n",
        "print(f\"- Input: English words\")\n",
        "print(f\"- Output: Context vector of size 32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e61963",
      "metadata": {
        "id": "b6e61963"
      },
      "source": [
        "## Step 5: Build Decoder\n",
        "\n",
        "* **Decoder Architecture:** This cell defines the decoder neural network that generates output sentences.\n",
        "\n",
        "**Hints:**\n",
        "- Decoder also uses Embedding layer and GRU\n",
        "- Add `nn.Linear` layer to project hidden state to vocabulary size\n",
        "- The decoder takes both input word and hidden state\n",
        "- Use `squeeze(1)` to remove extra dimension before linear layer\n",
        "\n",
        "**Documentation:**\n",
        "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "- [Tensor.squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99a9242-5fa6-4161-db04-ffa11f4dbad6",
        "id": "YBbm4_BPwX6g"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder created!\n",
            "- Input: Context vector + previous Hindi word\n",
            "- Output: Next Hindi word\n"
          ]
        }
      ],
      "source": [
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_size, vocab_size)\n",
        "        # Hint 1: Fully connected layer type\n",
        "        # Hint 2: Output dimension should be size of vocabulary (predicting next word)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # x: previous word, hidden: context from encoder\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        # Hint 3: Pass through GRU layer\n",
        "        # Hint 4: Second argument should be previous hidden state (from encoder or last step)\n",
        "\n",
        "        output = self.output(output.squeeze(1))\n",
        "        # Hint 5: Remove sequence dimension (since seq_len=1 during decoding)\n",
        "        # Hint 6: Use a tensor operation that removes a dimension of size 1\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "# Create decoder\n",
        "decoder = SimpleDecoder(\n",
        "    vocab_size=len(hin_vocab),\n",
        "    # Hint 7: Use Hindi vocabulary here\n",
        "\n",
        "    embed_size=16,\n",
        "    hidden_size=32\n",
        ")\n",
        "\n",
        "print(\"Decoder created!\")\n",
        "print(f\"- Input: Context vector + previous Hindi word\")\n",
        "print(f\"- Output: Next Hindi word\")"
      ],
      "id": "YBbm4_BPwX6g"
    },
    {
      "cell_type": "markdown",
      "id": "811e4731",
      "metadata": {
        "id": "811e4731"
      },
      "source": [
        "## Step 6: Train the Model\n",
        "\n",
        "* **Training Function:** This cell implements the training loop for one sentence pair.\n",
        "\n",
        "**Hints:**\n",
        "- Call `optimizer.zero_grad()` before each training step\n",
        "- Use `encoder(input)` to get context vector\n",
        "- Start decoding with SOS (Start Of Sentence) token\n",
        "- Use teacher forcing: feed correct previous word as input\n",
        "- Accumulate loss for each predicted word\n",
        "- Call `loss.backward()` to compute gradients\n",
        "- Call `optimizer.step()` to update weights\n",
        "\n",
        "**Documentation:**\n",
        "- [optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html)\n",
        "- [Tensor.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html)\n",
        "- [optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html)\n",
        "- [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0b5493",
      "metadata": {
        "id": "fe0b5493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2364723-13df-46dd-ac83-5aa00e706357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "\n",
            "Epoch  40: Loss = 0.008\n",
            "Epoch  80: Loss = 0.003\n",
            "Epoch 120: Loss = 0.001\n",
            "Epoch 160: Loss = 0.001\n",
            "Epoch 200: Loss = 0.001\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Training function\n",
        "def train_one_pair(eng_tensor, hin_tensor, encoder, decoder,\n",
        "                   enc_optimizer, dec_optimizer, criterion):\n",
        "    enc_optimizer.zero_grad()\n",
        "    # Hint 1: Reset gradients before backward pass\n",
        "\n",
        "    dec_optimizer.zero_grad()\n",
        "\n",
        "    # Encode English sentence\n",
        "    context = encoder(eng_tensor.unsqueeze(0))\n",
        "    # Hint 2: Add batch dimension (since model expects batch_first=True)\n",
        "\n",
        "    # Decode to Hindi\n",
        "    loss = 0\n",
        "    hidden = context\n",
        "    # Hint 3: Initialize decoder hidden state using encoder output (context vector)\n",
        "\n",
        "    for i in range(hin_tensor.shape[0]):\n",
        "        # Prepare input (previous word or SOS)\n",
        "        if i == 0:\n",
        "            dec_input = torch.tensor([[SOS]])\n",
        "            # Hint 4: Use Start-Of-Sentence token index\n",
        "        else:\n",
        "            dec_input = hin_tensor[i-1].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Predict next word\n",
        "        output, hidden = decoder(dec_input, hidden)\n",
        "        loss += criterion(output, hin_tensor[i].unsqueeze(0))\n",
        "\n",
        "    # Update weights\n",
        "    loss.backward()\n",
        "    # Hint 5: Compute gradients via backpropagation\n",
        "\n",
        "    enc_optimizer.step()\n",
        "    # Hint 6: Update encoder parameters\n",
        "\n",
        "    dec_optimizer.step()\n",
        "\n",
        "    return loss.item() / hin_tensor.size(0)\n",
        "\n",
        "# Setup training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Hint 7: Loss function used for multi-class classification (no softmax needed)\n",
        "\n",
        "enc_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.01)\n",
        "# Hint 8: Optimizer type (commonly used adaptive optimizer)\n",
        "\n",
        "dec_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01)\n",
        "\n",
        "# Train\n",
        "print(\"Training started...\\n\")\n",
        "for epoch in range(1, 201):\n",
        "    total_loss = 0\n",
        "    for eng_tensor, hin_tensor in pairs:\n",
        "        loss = train_one_pair(eng_tensor, hin_tensor, encoder, decoder,\n",
        "                             enc_optimizer, dec_optimizer, criterion)\n",
        "        total_loss += loss\n",
        "\n",
        "    if epoch % 40 == 0:\n",
        "        avg_loss = total_loss / len(pairs)\n",
        "        print(f\"Epoch {epoch:3d}: Loss = {avg_loss:.3f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4311fc5",
      "metadata": {
        "id": "d4311fc5"
      },
      "source": [
        "## Step 7: Test Translation\n",
        "\n",
        "* **Translation Function:** This cell implements the translation function to convert English to Hindi.\n",
        "\n",
        "**Hints:**\n",
        "- Use `torch.no_grad()` context to disable gradient computation during inference\n",
        "- Start with SOS token and predict word by word\n",
        "- Use `output.argmax(1)` to get the predicted word index\n",
        "- Stop when EOS token is predicted or max length reached\n",
        "- Use idx2word mapping to convert indices back to words\n",
        "\n",
        "**Documentation:**\n",
        "- [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html)\n",
        "- [Tensor.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5172cd9f",
      "metadata": {
        "id": "5172cd9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4b0c3c-1f78-4ebd-b34e-a87193e81700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation Results:\n",
            "============================================================\n",
            "English:   I am happy\n",
            "Predicted: मैं खुश हूं <EOS> ✗ wrong\n",
            "Actual:    मैं खुश हूं\n",
            "\n",
            "English:   you are good\n",
            "Predicted: तुम अच्छे हो <EOS> ✗ wrong\n",
            "Actual:    तुम अच्छे हो\n",
            "\n",
            "English:   he is smart\n",
            "Predicted: वह होशियार है <EOS> ✗ wrong\n",
            "Actual:    वह होशियार है\n",
            "\n",
            "English:   she is kind\n",
            "Predicted: वह दयालु है <EOS> ✗ wrong\n",
            "Actual:    वह दयालु है\n",
            "\n",
            "English:   we are friends\n",
            "Predicted: हम दोस्त हैं <EOS> ✗ wrong\n",
            "Actual:    हम दोस्त हैं\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def translate(sentence, encoder, decoder, eng_vocab, hin_idx2word, max_len=10):\n",
        "    \"\"\"Translate English sentence to Hindi\"\"\"\n",
        "    # Convert to indices\n",
        "    eng_tensor = sentence_to_indices(sentence, eng_vocab)\n",
        "\n",
        "    # Encode\n",
        "    with torch.no_grad():\n",
        "        # Hint 1: Disable gradient computation during inference\n",
        "\n",
        "        context = encoder(eng_tensor.unsqueeze(0))\n",
        "\n",
        "    # Decode word by word\n",
        "    words = []\n",
        "    hidden = context\n",
        "    dec_input = torch.tensor([[SOS]])\n",
        "    # Hint 2: Start decoding with Start-Of-Sentence token index (SOS)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = decoder(dec_input, hidden)\n",
        "\n",
        "        # Get predicted word\n",
        "        predicted_idx = output.argmax(1).item()\n",
        "        # Hint 3: Function that returns index of highest probability along vocab dimension\n",
        "\n",
        "        if predicted_idx == dec_input:\n",
        "            # Hint 4: Stop when End-Of-Sentence token is generated\n",
        "            break\n",
        "\n",
        "        word = hin_idx2word[predicted_idx]\n",
        "        words.append(word)\n",
        "        # Hint 5: Add predicted word to list\n",
        "\n",
        "        dec_input = torch.tensor([[predicted_idx]])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Test on training examples\n",
        "print(\"Translation Results:\")\n",
        "print(\"=\" * 60)\n",
        "for eng, hin in data:\n",
        "    predicted = translate(eng, encoder, decoder, eng_vocab, hin_idx2word)\n",
        "    match = \"✓ correct\" if predicted == hin else \"✗ wrong\"\n",
        "    print(f\"English:   {eng}\")\n",
        "    print(f\"Predicted: {predicted} {match}\")\n",
        "    print(f\"Actual:    {hin}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0454925d",
      "metadata": {
        "id": "0454925d"
      },
      "source": [
        "## Try Your Own Translations\n",
        "\n",
        "* **Test Custom Input:** This cell allows you to test the model with your own English sentences.\n",
        "\n",
        "**Hints:**\n",
        "- Modify the test_sentence variable to try different inputs\n",
        "- The model will warn you about words not in the vocabulary\n",
        "- Unknown words will be replaced with UNK token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11e5526",
      "metadata": {
        "id": "f11e5526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2875deeb-42fc-424c-8c21-cddf2fb290e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  I am sad\n",
            "⚠ Unknown words (will be replaced with <UNK>): ['sad']\n",
            "\n",
            "Available words: ['I', 'am', 'happy', 'you', 'are', 'good', 'he', 'is', 'smart', 'she', 'kind', 'we', 'friends']\n",
            "Output: मैं खुश हूं <EOS>\n"
          ]
        }
      ],
      "source": [
        "# Test translation\n",
        "test_sentence = \"I am sad\"  # Change this!\n",
        "\n",
        "print(f\"Input:  {test_sentence}\")\n",
        "\n",
        "# Check for unknown words\n",
        "words = test_sentence.split()\n",
        "unknown_words = [w for w in words if w not in eng_vocab]\n",
        "if unknown_words:\n",
        "    print(f\"⚠ Unknown words (will be replaced with <UNK>): {unknown_words}\")\n",
        "\n",
        "print(f\"\\nAvailable words: {list(eng_vocab.keys())[4:]}\")  # Skip special tokens\n",
        "\n",
        "output = translate(test_sentence, encoder, decoder, eng_vocab, hin_idx2word)\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "768377ea",
      "metadata": {
        "id": "768377ea"
      },
      "source": [
        "---\n",
        "\n",
        "## How It Works - Summary\n",
        "\n",
        "### 1. Encoder\n",
        "- Reads input sentence word by word\n",
        "- Creates a fixed-size \"summary\" (context vector)\n",
        "- This vector captures the meaning\n",
        "\n",
        "### 2. Decoder\n",
        "- Starts with the context vector\n",
        "- Generates output one word at a time\n",
        "- Each word depends on context + previous words\n",
        "\n",
        "### 3. Training\n",
        "- Model learns by comparing predictions to correct translations\n",
        "- Adjusts weights to minimize errors\n",
        "- After many iterations, learns the translation pattern\n",
        "\n",
        "### Limitations\n",
        "- Only works with words seen during training\n",
        "- Limited vocabulary (5 sentence pairs)\n",
        "- No attention mechanism (can't focus on specific input words)\n",
        "\n",
        "### To Improve\n",
        "- Add more training data\n",
        "- Implement attention mechanism\n",
        "- Use larger embedding and hidden sizes\n",
        "- Train for more epochs\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "Learn more about encoder-decoder architectures:\n",
        "- [Sequence to Sequence Learning](https://arxiv.org/abs/1409.3215)\n",
        "- [PyTorch Seq2Seq Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282cc024",
      "metadata": {
        "id": "282cc024"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}